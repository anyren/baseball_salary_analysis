{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85420be1",
   "metadata": {},
   "source": [
    "# Predicting Salaries of MLB Players\n",
    "\n",
    "- We found a dataset on kaggle.com of MLB position player statistics and salary data (adjusted for inflation) for 1985-2016. (https://www.kaggle.com/datasets/andrewdecker/hitters-salary-adjusted-to-inflation)\n",
    "\n",
    "- The dataset comprises **15023 observations with 29 features (4 ID features, 2 salary features, 4 fielding features, 19 offensive features)**. \n",
    "\n",
    "- We focused on **ADJ Salary** (salary column adjusted for inflation) as our dependent variable.\n",
    "\n",
    "- After cleaning the dataset, we had **15014 observations, 25 independent variables, 1 dependent variable**.\n",
    "\n",
    "- Based on a suggestion in a Moneyball-themed post on Medium.com, we transformed the ADJ Salary column into its natural logarithm, thereby making the histogram distribution look more like a normal, Gaussian distribution. (https://medium.com/towards-data-science/did-the-money-follow-the-ball-analyzing-the-importance-of-baseball-batting-statistics-pre-144d7d452e1f)\n",
    "\n",
    "- Correlation matrix analysis revealed that the offensive features were more highly correlated with our dependent variable than any of the other features in the dataset, so we focused our efforts there. **GS (games started), BB (walks), RBI (runs batted in), R (runs scored), HR (home runs), and InnOuts (inning outs, a measure of game time played)** were the highest-correlated with ADJ Salary. \n",
    "\n",
    "- A pairplot of these offensive features didn't show an obvious linear relationship with ADJ Salary, but unfortunately quite a bit of collinearity with each other.  \n",
    "\n",
    "- Scatter plots between each feature of interest (GS, BB, RBI, R, HR, InnOuts) and the ADJ Salary dependent variable didn't reveal any obvious linear relationship.\n",
    "\n",
    "- Simple univariate linear regression was conducted using LinearRegression(), GradientBoostingRegressor(), RandomForestRegressor(), and statsmodels ols regressor for each feature of interest. The regressors were unable to explain more than **20% of the variance in the dependent variable**. \n",
    "\n",
    "- Our next step was to attempt a multivariate linear regression with all of the features of interest, using the aforementioned regressors. This time the results were marginally better: the multivariate linear regressions explain approximately **19-26% of variance in ADJ Salary**.  \n",
    "\n",
    "- We were disappointed with these results, so we took a different approach. What if we aggregated the dataset, and took the mean values of all of a player's stats over his career (including salary)? Could a multivariate linear regression on this aggregation perform any better?\n",
    "\n",
    "- We grouped the dataset by playerID, took the mean of every feature, and ended up with **2468 observations**.\n",
    "\n",
    "- Correlation analysis, pairplot visualization, and scatter plots now revealed stronger correlations and linear relationships between **RBI, R, twoB (doubles)** and ADJ Salary, but again with lots of multicollinearity. \n",
    "\n",
    "- Simple linear regressions for each new feature of interest and our dependent variable now showed over **50% explained variance**.\n",
    "\n",
    "- Now a return to multivariate linear regression on these new features yielded similar values, achieving **59% explained variance** on the GradientBoostingRegressor() model. (GBR1 Testing Score: \t0.5907526209843459)\n",
    "\n",
    "- We attempted to improve the scores using ensemble methods (Ridge and ElasticNet), but they did not improve upon the previous results. \n",
    "\n",
    "- Nevertheless, our methods demonstrated substantial improvement in explained variance in our dependent variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d7c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224141ad",
   "metadata": {},
   "source": [
    "# EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f586b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df4e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Hitters_Adjusted_Salary.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf5aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "\n",
    "df = df.drop(columns=[\"Unnamed: 0\", \"salary\", \"teamID\", \"lgID\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e7699",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3082546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31bea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a0736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make ADJ Salary into integer\n",
    "\n",
    "df[\"ADJ Salary\"] = df[\"ADJ Salary\"].astype(\"int\").round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84226275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with 0 salary\n",
    "\n",
    "df = df.loc[(df[\"ADJ Salary\"] > 0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the natural logarithm of the salary column\n",
    "\n",
    "df[\"ADJ Salary\"] = np.log(df[\"ADJ Salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f200866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"2B\":\"twoB\", \"3B\":\"threeB\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb369cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15014 observations, 25 independent variables, 1 dependent variable\n",
    "\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250373c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at distributions of the variables\n",
    "\n",
    "df.hist(figsize = (15, 15))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27bd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix reveals the best independent variables: RBI, BB, GS, R, HR \n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "cols = [\"ADJ Salary\",\"GS\",\"InnOuts\",\"PO\",\"A\",\"E\",\"DP\",\"G\",\"AB\",\"R\",\\\n",
    "        \"H\",\"twoB\",\"threeB\",\"HR\",\"RBI\",\"SB\",\"CS\",\"BB\",\"SO\",\"IBB\",\"HBP\",\"SH\",\"SF\",\"GIDP\"]\n",
    "\n",
    "corr = df[cols].corr()\n",
    "corr = corr.style.background_gradient(cmap='Blues')\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d658ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for multicollinearity\n",
    "\n",
    "sns.pairplot(df[[\"ADJ Salary\",\"RBI\",\"BB\",\"GS\",\"R\",\"HR\",\"InnOuts\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[[\"GS\",\"BB\",\"RBI\",\"R\",\"HR\",\"InnOuts\"]].corr()\n",
    "corr = corr.style.background_gradient(cmap='Purples')\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at scatterplots\n",
    "\n",
    "plt.scatter(df[\"GS\"][:1000], df[\"ADJ Salary\"][:1000])\n",
    "plt.xlabel(\"GS\")\n",
    "plt.ylabel(\"ADJ Salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146bc30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"BB\"][:1000], df[\"ADJ Salary\"][:1000])\n",
    "plt.xlabel(\"BB\")\n",
    "plt.ylabel(\"ADJ Salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5642f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"RBI\"][:1000], df[\"ADJ Salary\"][:1000])\n",
    "plt.xlabel(\"RBI\")\n",
    "plt.ylabel(\"ADJ Salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca61f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"R\"][:1000], df[\"ADJ Salary\"][:1000])\n",
    "plt.xlabel(\"R\")\n",
    "plt.ylabel(\"ADJ Salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"HR\"][:1000], df[\"ADJ Salary\"][:1000])\n",
    "plt.xlabel(\"HR\")\n",
    "plt.ylabel(\"ADJ Salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"InnOuts\"][:1000], df[\"ADJ Salary\"][:1000])\n",
    "plt.xlabel(\"InnOuts\")\n",
    "plt.ylabel(\"ADJ Salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a63f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform linear regression on each candidate independent variable\n",
    "\n",
    "def simple_LR(a_df, col_list):\n",
    "            \n",
    "    # Assign X and y\n",
    "\n",
    "    X = a_df[col_list]\n",
    "\n",
    "    # X = df.drop(columns=[\"ADJ Salary\", \"playerID\"])\n",
    "\n",
    "    y = a_df[\"ADJ Salary\"]\n",
    "\n",
    "    # Split the data into X_train, X_test, y_train, y_test\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "\n",
    "    # Create a scaler to standardize the data\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Train the scaler with the X_train data.\n",
    "\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # Transform X_train and X_test.\n",
    "\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    LR1 = LinearRegression().fit(X_train_scaled, y_train)\n",
    "    GBR1 = GradientBoostingRegressor().fit(X_train_scaled, y_train)\n",
    "    RFR1 = RandomForestRegressor().fit(X_train_scaled, y_train)\n",
    "\n",
    "    LR1_pred = LR1.predict(X_test)\n",
    "    GBR1_pred = GBR1.predict(X_test)\n",
    "    RFR1_pred = RFR1.predict(X_test)\n",
    "\n",
    "    LR1_mse = mean_squared_error(y_test, LR1_pred)\n",
    "    GBR1_mse = mean_squared_error(y_test, GBR1_pred)\n",
    "    RFR1_mse = mean_squared_error(y_test, RFR1_pred)\n",
    "\n",
    "    LR1_r2 = r2_score(y_test, LR1_pred)\n",
    "    GBR1_r2 = r2_score(y_test, GBR1_pred)\n",
    "    RFR1_r2 = r2_score(y_test, RFR1_pred)\n",
    "\n",
    "    # Score the regression models\n",
    "\n",
    "    print(f\"LR1 Training Score: \\t\\t{LR1.score(X_train_scaled, y_train)}\")\n",
    "    print(f\"LR1 Testing Score: \\t{LR1.score(X_test_scaled, y_test)}\")\n",
    "    print(f\"LR1 r2: \\t\\t\\t{LR1_r2}\")\n",
    "    print(f\"LR1 mse: \\t\\t\\t{LR1_mse}\\n\")\n",
    "\n",
    "    print(f\"GBR1 Training Score: \\t\\t{GBR1.score(X_train_scaled, y_train)}\")\n",
    "    print(f\"GBR1 Testing Score: \\t{GBR1.score(X_test_scaled, y_test)}\")\n",
    "    print(f\"GBR1 r2: \\t\\t\\t{GBR1_r2}\")\n",
    "    print(f\"GBR1 mse: \\t\\t\\t{GBR1_mse}\\n\")\n",
    "\n",
    "    print(f\"RFR1 Training Score: \\t\\t{RFR1.score(X_train_scaled, y_train)}\")\n",
    "    print(f\"RFR1 Testing Score: \\t{RFR1.score(X_test_scaled, y_test)}\")\n",
    "    print(f\"RFR1 r2: \\t\\t\\t{RFR1_r2}\")\n",
    "    print(f\"RFR1 mse: \\t\\t\\t{RFR1_mse}\\n\")\n",
    "\n",
    "    formula = f'y ~ {\" + \".join(c for c in col_list)}'\n",
    "\n",
    "    LR1_stats = smf.ols(formula=formula, data=X).fit()\n",
    "\n",
    "    print(LR1_stats.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f2c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LR(df, [\"RBI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35253286",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LR(df, [\"GS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LR(df, [\"BB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LR(df, [\"R\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LR(df, [\"HR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LR(df, [\"InnOuts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f95acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"first_predictions_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9acc9ee",
   "metadata": {},
   "source": [
    "## Results:\n",
    "\n",
    "- all the highly correlated independent vars are also correlated with each other = multicollinearity!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e851823",
   "metadata": {},
   "source": [
    "## First attempt at multivariate linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da44894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X and y\n",
    "\n",
    "X = df[[\"RBI\", \"BB\", \"GS\", \"R\", \"HR\"]]\n",
    "\n",
    "y = df[\"ADJ Salary\"]\n",
    "\n",
    "# Split the data into X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "\n",
    "# Create a scaler to standardize the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Train the scaler with the X_train data.\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform X_train and X_test.\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "LR1 = LinearRegression().fit(X_train_scaled, y_train)\n",
    "GBR1 = GradientBoostingRegressor().fit(X_train_scaled, y_train)\n",
    "RFR1 = RandomForestRegressor().fit(X_train_scaled, y_train)\n",
    "\n",
    "LR1_pred = LR1.predict(X_test)\n",
    "GBR1_pred = GBR1.predict(X_test)\n",
    "RFR1_pred = RFR1.predict(X_test)\n",
    "\n",
    "LR1_mse = mean_squared_error(y_test, LR1_pred)\n",
    "GBR1_mse = mean_squared_error(y_test, GBR1_pred)\n",
    "RFR1_mse = mean_squared_error(y_test, RFR1_pred)\n",
    "\n",
    "LR1_r2 = r2_score(y_test, LR1_pred)\n",
    "GBR1_r2 = r2_score(y_test, GBR1_pred)\n",
    "RFR1_r2 = r2_score(y_test, RFR1_pred)\n",
    "\n",
    "# Score the regression models\n",
    "\n",
    "print(f\"LR1 Training Score: \\t\\t{LR1.score(X_train_scaled, y_train)}\")\n",
    "print(f\"LR1 Testing Score: \\t{LR1.score(X_test_scaled, y_test)}\")\n",
    "print(f\"LR1 r2: \\t\\t\\t{LR1_r2}\")\n",
    "print(f\"LR1 mse: \\t\\t\\t{LR1_mse}\\n\")\n",
    "\n",
    "print(f\"GBR1 Training Score: \\t\\t{GBR1.score(X_train_scaled, y_train)}\")\n",
    "print(f\"GBR1 Testing Score: \\t{GBR1.score(X_test_scaled, y_test)}\")\n",
    "print(f\"GBR1 r2: \\t\\t\\t{GBR1_r2}\")\n",
    "print(f\"GBR1 mse: \\t\\t\\t{GBR1_mse}\\n\")\n",
    "\n",
    "print(f\"RFR1 Training Score: \\t\\t{RFR1.score(X_train_scaled, y_train)}\")\n",
    "print(f\"RFR1 Testing Score: \\t{RFR1.score(X_test_scaled, y_test)}\")\n",
    "print(f\"RFR1 r2: \\t\\t\\t{RFR1_r2}\")\n",
    "print(f\"RFR1 mse: \\t\\t\\t{RFR1_mse}\\n\")\n",
    "\n",
    "\n",
    "LR1_stats = smf.ols(formula = \"y ~ RBI + BB + GS + R + HR\", data=X).fit()\n",
    "\n",
    "LR1_stats.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15171d62",
   "metadata": {},
   "source": [
    "## Results: \n",
    "- the multivariate linear regressions explain approximately 19-26% of variance\n",
    "- we can do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb0048",
   "metadata": {},
   "source": [
    "## Second attempt at multivariate linear regression ... a more savvy approach this time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm now aggregating the data across players' careers, taking the mean of all variables\n",
    "\n",
    "df = pd.read_csv(\"first_predictions_df.csv\", index_col=\"Unnamed: 0\")\n",
    "\n",
    "agg_df = df.groupby([\"playerID\"]).mean()\n",
    "agg_df.to_csv(\"second_predictions_df.csv\")\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at distributions of the variables\n",
    "\n",
    "agg_df.hist(figsize = (15, 15))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix reveals the best independent variables: RBI, H, R, 2B\n",
    "\n",
    "cols = [\"ADJ Salary\",\"GS\",\"InnOuts\",\"PO\",\"A\",\"E\",\"DP\",\"G\",\"AB\",\"R\",\\\n",
    "        \"H\",\"twoB\",\"threeB\",\"HR\",\"RBI\",\"SB\",\"CS\",\"BB\",\"SO\",\"IBB\",\"HBP\",\"SH\",\"SF\",\"GIDP\"]\n",
    "\n",
    "corr = agg_df[cols].corr()\n",
    "corr = corr.style.background_gradient(cmap='Purples')\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12894726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for multicollinearity\n",
    "\n",
    "sns.pairplot(agg_df[[\"ADJ Salary\",\"RBI\", \"R\", \"twoB\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47647403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lots of multicollinearity, but all these vars plot a linear relationship with ADJ Salary\n",
    "\n",
    "corr = agg_df[[\"ADJ Salary\",\"RBI\", \"R\", \"twoB\"]].corr()\n",
    "corr = corr.style.background_gradient(cmap='Purples')\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(agg_df[\"RBI\"], agg_df[\"ADJ Salary\"])\n",
    "plt.xlabel(\"RBI\")\n",
    "plt.ylabel(\"ADJ Salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10eb07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(agg_df[\"R\"], agg_df[\"ADJ Salary\"])\n",
    "plt.xlabel(\"R\")\n",
    "plt.ylabel(\"ADJ Salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65636939",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(agg_df[\"twoB\"], agg_df[\"ADJ Salary\"])\n",
    "plt.xlabel(\"twoB\")\n",
    "plt.ylabel(\"ADJ Salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d56f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LR(agg_df, [\"RBI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd34499",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LR(agg_df, [\"R\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_LR(agg_df, [\"twoB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2369deb",
   "metadata": {},
   "source": [
    "## Let's try multivariate linear regression again, this time on the aggregated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424756fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = pd.read_csv(\"second_predictions_df.csv\", index_col=\"playerID\")\n",
    "agg_df\n",
    "\n",
    "# Assign X and y\n",
    "\n",
    "X = agg_df[[\"RBI\", \"R\", \"twoB\"]]\n",
    "\n",
    "y = agg_df[\"ADJ Salary\"]\n",
    "\n",
    "# Split the data into X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "\n",
    "# Create a scaler to standardize the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Train the scaler with the X_train data.\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform X_train and X_test.\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "LR1 = LinearRegression().fit(X_train_scaled, y_train)\n",
    "GBR1 = GradientBoostingRegressor().fit(X_train_scaled, y_train)\n",
    "RFR1 = RandomForestRegressor().fit(X_train_scaled, y_train)\n",
    "\n",
    "LR1_pred = LR1.predict(X_test)\n",
    "GBR1_pred = GBR1.predict(X_test)\n",
    "RFR1_pred = RFR1.predict(X_test)\n",
    "\n",
    "LR1_mse = mean_squared_error(y_test, LR1_pred)\n",
    "GBR1_mse = mean_squared_error(y_test, GBR1_pred)\n",
    "RFR1_mse = mean_squared_error(y_test, RFR1_pred)\n",
    "\n",
    "LR1_r2 = r2_score(y_test, LR1_pred)\n",
    "GBR1_r2 = r2_score(y_test, GBR1_pred)\n",
    "RFR1_r2 = r2_score(y_test, RFR1_pred)\n",
    "\n",
    "# Score the regression models\n",
    "\n",
    "print(f\"LR1 Training Score: \\t\\t{LR1.score(X_train_scaled, y_train)}\")\n",
    "print(f\"LR1 Testing Score: \\t{LR1.score(X_test_scaled, y_test)}\")\n",
    "print(f\"LR1 r2: \\t\\t\\t{LR1_r2}\")\n",
    "print(f\"LR1 mse: \\t\\t\\t{LR1_mse}\\n\")\n",
    "\n",
    "print(f\"GBR1 Training Score: \\t\\t{GBR1.score(X_train_scaled, y_train)}\")\n",
    "print(f\"GBR1 Testing Score: \\t{GBR1.score(X_test_scaled, y_test)}\")\n",
    "print(f\"GBR1 r2: \\t\\t\\t{GBR1_r2}\")\n",
    "print(f\"GBR1 mse: \\t\\t\\t{GBR1_mse}\\n\")\n",
    "\n",
    "print(f\"RFR1 Training Score: \\t\\t{RFR1.score(X_train_scaled, y_train)}\")\n",
    "print(f\"RFR1 Testing Score: \\t{RFR1.score(X_test_scaled, y_test)}\")\n",
    "print(f\"RFR1 r2: \\t\\t\\t{RFR1_r2}\")\n",
    "print(f\"RFR1 mse: \\t\\t\\t{RFR1_mse}\\n\")\n",
    "\n",
    "# LR1_stats = smf.ols(formula = 'y ~ yearID + GS + InnOuts + PO + A + E + DP + G + AB + R + H +\\\n",
    "# twoB + threeB + HR + RBI + SB + CS + BB + SO + IBB + HBP + SH + SF + GIDP', data=X).fit()\n",
    "\n",
    "LR1_stats = smf.ols(formula = \"y ~ RBI + R + twoB\", data=X).fit()\n",
    "                    \n",
    "LR1_stats.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4cb9f",
   "metadata": {},
   "source": [
    "## Results: \n",
    "- the linear regressions on the aggregated data now explain nearly 60% of variance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ab82c",
   "metadata": {},
   "source": [
    "# Linear Regression Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegression()\n",
    "\n",
    "agg_df = pd.read_csv(\"second_predictions_df.csv\", index_col=\"playerID\")\n",
    "agg_df\n",
    "\n",
    "# Assign X and y\n",
    "\n",
    "X = agg_df[[\"RBI\",\"R\",\"twoB\"]]\n",
    "\n",
    "y = agg_df[\"ADJ Salary\"]\n",
    "\n",
    "# Split the data into X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "\n",
    "# Create a scaler to standardize the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Train the scaler with the X_train data.\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform X_train and X_test.\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LinearRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "# Score the model\n",
    "\n",
    "print(f\"STDSCALER Linear Regression Score: {model.score(X_train_scaled, y_train)}\")\n",
    "print(f\"STDSCALER Linear Regression Score: {model.score(X_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6a3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = pd.read_csv(\"second_predictions_df.csv\", index_col=\"playerID\")\n",
    "agg_df\n",
    "\n",
    "# Assign X and y\n",
    "\n",
    "X = agg_df[[\"RBI\",\"R\",\"twoB\"]]\n",
    "\n",
    "y = agg_df[\"ADJ Salary\"]\n",
    "\n",
    "# Split the data into X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "\n",
    "# Create a scaler to standardize the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Train the scaler with the X_train data.\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform X_train and X_test.\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "ridge_reg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "predicted = ridge_reg.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, predicted)\n",
    "r2 = r2_score(y_test, predicted)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"R2: {r2}\") \n",
    "\n",
    "# Score the model\n",
    "\n",
    "print(f\"Ridge Regression Score: {ridge_reg.score(X_train_scaled, y_train)}\")\n",
    "print(f\"Ridge Regression Score: {ridge_reg.score(X_test_scaled, y_test)}\")\n",
    "\n",
    "plt.bar(np.arange(len(ridge_reg.coef_)), ridge_reg.coef_)\n",
    "plt.title(f'Ridge Regression coefficient plot')\n",
    "plt.show()     \n",
    "\n",
    "sel = SelectFromModel(ridge_reg)\n",
    "sel.fit(X_train_scaled, y_train)\n",
    "SelectFromModel(estimator=Ridge())\n",
    "\n",
    "X_selected_train, X_selected_test, y_train, y_test = train_test_split(sel.transform(X), y, random_state=1)\n",
    "\n",
    "scaler = StandardScaler().fit(X_selected_train)\n",
    "\n",
    "X_selected_train_scaled = scaler.transform(X_selected_train)\n",
    "X_selected_test_scaled = scaler.transform(X_selected_test)\n",
    "\n",
    "new_ridge_reg = LinearRegression().fit(X_selected_train_scaled, y_train)\n",
    "print(f\"New ridge regression score: {new_ridge_reg.score(X_selected_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff659046",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = pd.read_csv(\"second_predictions_df.csv\", index_col=\"playerID\")\n",
    "agg_df\n",
    "\n",
    "# Assign X and y\n",
    "\n",
    "\n",
    "X = agg_df[[\"RBI\",\"R\",\"twoB\"]]\n",
    "\n",
    "y = agg_df[\"ADJ Salary\"]\n",
    "\n",
    "# Split the data into X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "\n",
    "# Create a scaler to standardize the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Train the scaler with the X_train data.\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform X_train and X_test.\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "elasticnet_reg = ElasticNet().fit(X_train, y_train)\n",
    "\n",
    "predicted = elasticnet_reg.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, predicted)\n",
    "r2 = r2_score(y_test, predicted)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"R2: {r2}\") \n",
    "\n",
    "# Score the model\n",
    "\n",
    "print(f\"ElasticNet Regression Score: {elasticnet_reg.score(X_train_scaled, y_train)}\")\n",
    "print(f\"ElasticNet Regression Score: {elasticnet_reg.score(X_test_scaled, y_test)}\")\n",
    "\n",
    "plt.bar(np.arange(len(elasticnet_reg.coef_)), elasticnet_reg.coef_)\n",
    "plt.title(f'ElasticNet Regression coefficient plot')\n",
    "plt.show()  \n",
    "\n",
    "sel = SelectFromModel(elasticnet_reg)\n",
    "sel.fit(X_train_scaled, y_train)\n",
    "SelectFromModel(estimator=ElasticNet())\n",
    "\n",
    "X_selected_train, X_selected_test, y_train, y_test = train_test_split(sel.transform(X), y, random_state=1)\n",
    "\n",
    "scaler = StandardScaler().fit(X_selected_train)\n",
    "\n",
    "X_selected_train_scaled = scaler.transform(X_selected_train)\n",
    "X_selected_test_scaled = scaler.transform(X_selected_test)\n",
    "\n",
    "new_elasticnet_reg = LinearRegression().fit(X_selected_train_scaled, y_train)\n",
    "print(f\"New linear regression score: {new_elasticnet_reg.score(X_selected_test_scaled, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
